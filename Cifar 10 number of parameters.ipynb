{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg \n",
    "import numpy as np\n",
    "\n",
    "from scipy import misc\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "para=[]\n",
    "train_loss_list=[]\n",
    "train_acc_list=[]\n",
    "test_loss_list=[]\n",
    "test_acc_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 32, 32]             304\n",
      "       BatchNorm2d-2            [-1, 4, 32, 32]               8\n",
      "              ReLU-3            [-1, 4, 32, 32]               0\n",
      "         MaxPool2d-4            [-1, 4, 16, 16]               0\n",
      "            Conv2d-5            [-1, 4, 16, 16]             404\n",
      "       BatchNorm2d-6            [-1, 4, 16, 16]               8\n",
      "              ReLU-7            [-1, 4, 16, 16]               0\n",
      "         MaxPool2d-8              [-1, 4, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 3,294\n",
      "Trainable params: 3,294\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.13\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 4, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1.3079991162109375\n",
      "0.53518\n",
      "1.2973865028381348\n",
      "0.5361\n"
     ]
    }
   ],
   "source": [
    "para.append(3294)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==5:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)\n",
    "        test_acc_list.append(test_correct/test_total)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 32, 32]             304\n",
      "       BatchNorm2d-2            [-1, 4, 32, 32]               8\n",
      "              ReLU-3            [-1, 4, 32, 32]               0\n",
      "         MaxPool2d-4            [-1, 4, 16, 16]               0\n",
      "            Conv2d-5            [-1, 8, 16, 16]             808\n",
      "       BatchNorm2d-6            [-1, 8, 16, 16]              16\n",
      "              ReLU-7            [-1, 8, 16, 16]               0\n",
      "         MaxPool2d-8              [-1, 8, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 6,266\n",
      "Trainable params: 6,266\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.15\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 4, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1.2007359145355225\n",
      "0.57762\n",
      "1.1842458433151246\n",
      "0.5877\n"
     ]
    }
   ],
   "source": [
    "para.append(6266)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)\n",
    "        test_acc_list.append(test_correct/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 32, 32]             304\n",
      "       BatchNorm2d-2            [-1, 4, 32, 32]               8\n",
      "              ReLU-3            [-1, 4, 32, 32]               0\n",
      "         MaxPool2d-4            [-1, 4, 16, 16]               0\n",
      "            Conv2d-5           [-1, 16, 16, 16]           1,616\n",
      "       BatchNorm2d-6           [-1, 16, 16, 16]              32\n",
      "              ReLU-7           [-1, 16, 16, 16]               0\n",
      "         MaxPool2d-8             [-1, 16, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]          10,250\n",
      "================================================================\n",
      "Total params: 12,210\n",
      "Trainable params: 12,210\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.20\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 0.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 4, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1.1217647747039794\n",
      "0.60774\n",
      "1.1469328819274902\n",
      "0.5926\n"
     ]
    }
   ],
   "source": [
    "para.append(12210)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)\n",
    "        test_acc_list.append(test_correct/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 32, 32]             304\n",
      "       BatchNorm2d-2            [-1, 4, 32, 32]               8\n",
      "              ReLU-3            [-1, 4, 32, 32]               0\n",
      "         MaxPool2d-4            [-1, 4, 16, 16]               0\n",
      "            Conv2d-5           [-1, 32, 16, 16]           3,232\n",
      "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
      "              ReLU-7           [-1, 32, 16, 16]               0\n",
      "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 24,098\n",
      "Trainable params: 24,098\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.30\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 0.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 4, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1.051061689529419\n",
      "0.6356\n",
      "1.0810808820724487\n",
      "0.6219\n"
     ]
    }
   ],
   "source": [
    "para.append(24098)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)\n",
    "        test_acc_list.append(test_correct/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 32, 32]             608\n",
      "       BatchNorm2d-2            [-1, 8, 32, 32]              16\n",
      "              ReLU-3            [-1, 8, 32, 32]               0\n",
      "         MaxPool2d-4            [-1, 8, 16, 16]               0\n",
      "            Conv2d-5           [-1, 32, 16, 16]           6,432\n",
      "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
      "              ReLU-7           [-1, 32, 16, 16]               0\n",
      "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 27,610\n",
      "Trainable params: 27,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.41\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 0.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.9191163285827637\n",
      "0.68438\n",
      "0.9492434027671814\n",
      "0.6781\n"
     ]
    }
   ],
   "source": [
    "para.append(27610)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)\n",
    "        test_acc_list.append(test_correct/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]           1,216\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 16, 16, 16]               0\n",
      "            Conv2d-5           [-1, 32, 16, 16]          12,832\n",
      "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
      "              ReLU-7           [-1, 32, 16, 16]               0\n",
      "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 34,634\n",
      "Trainable params: 34,634\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.61\n",
      "Params size (MB): 0.13\n",
      "Estimated Total Size (MB): 0.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.8918982452011108\n",
      "0.69078\n",
      "0.8964565420150756\n",
      "0.6906\n"
     ]
    }
   ],
   "source": [
    "para.append(34634)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)\n",
    "        test_acc_list.append(test_correct/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]           2,432\n",
      "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
      "              ReLU-3           [-1, 32, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
      "            Conv2d-5           [-1, 32, 16, 16]          25,632\n",
      "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
      "              ReLU-7           [-1, 32, 16, 16]               0\n",
      "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 48,682\n",
      "Trainable params: 48,682\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.02\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 1.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.8482725132369995\n",
      "0.70684\n",
      "0.9210660522460937\n",
      "0.6811\n"
     ]
    }
   ],
   "source": [
    "para.append(48682)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]           2,432\n",
      "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
      "              ReLU-3           [-1, 32, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]          51,264\n",
      "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
      "              ReLU-7           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-8             [-1, 64, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 94,858\n",
      "Trainable params: 94,858\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.22\n",
      "Params size (MB): 0.36\n",
      "Estimated Total Size (MB): 1.59\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(4096, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.7772547065544129\n",
      "0.73252\n",
      "0.8575447271347046\n",
      "0.7019\n"
     ]
    }
   ],
   "source": [
    "para.append(94858)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           4,864\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]         102,464\n",
      "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
      "              ReLU-7           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-8             [-1, 64, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 148,554\n",
      "Trainable params: 148,554\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.03\n",
      "Params size (MB): 0.57\n",
      "Estimated Total Size (MB): 2.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(4096, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.7799838606262207\n",
      "0.73134\n",
      "0.8162619127273559\n",
      "0.7217\n"
     ]
    }
   ],
   "source": [
    "para.append(148554)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 32, 32]           9,728\n",
      "       BatchNorm2d-2          [-1, 128, 32, 32]             256\n",
      "              ReLU-3          [-1, 128, 32, 32]               0\n",
      "         MaxPool2d-4          [-1, 128, 16, 16]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]         204,864\n",
      "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
      "              ReLU-7           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-8             [-1, 64, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 255,946\n",
      "Trainable params: 255,946\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.66\n",
      "Params size (MB): 0.98\n",
      "Estimated Total Size (MB): 4.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(4096, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.7740344680595398\n",
      "0.73402\n",
      "0.8179407058715821\n",
      "0.7149\n"
     ]
    }
   ],
   "source": [
    "para.append(255946)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 32, 32]           9,728\n",
      "       BatchNorm2d-2          [-1, 128, 32, 32]             256\n",
      "              ReLU-3          [-1, 128, 32, 32]               0\n",
      "         MaxPool2d-4          [-1, 128, 16, 16]               0\n",
      "            Conv2d-5          [-1, 128, 16, 16]         409,728\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "         MaxPool2d-8            [-1, 128, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]          81,930\n",
      "================================================================\n",
      "Total params: 501,898\n",
      "Trainable params: 501,898\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.06\n",
      "Params size (MB): 1.91\n",
      "Estimated Total Size (MB): 5.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "           )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(4096*2, num_classes)\n",
    "        \n",
    "    # 定义前向传播顺序\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.7503058015918732\n",
      "0.74248\n",
      "0.7544883083343505\n",
      "0.7443\n"
     ]
    }
   ],
   "source": [
    "para.append(501898)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    test_total=0\n",
    "    test_correct=0.0\n",
    "    test_loss=0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # 注意模型在GPU中，数据也要搬到GPU中\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    if (epoch+1)==3:\n",
    "        \n",
    "        for i,(images, labels) in enumerate(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss=criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(epoch+1)\n",
    "        print(running_loss/len(trainset))\n",
    "        print(running_corrects.double().item()/len(trainset))\n",
    "        print(test_loss/test_total)\n",
    "        print(test_correct/test_total)\n",
    "        \n",
    "        train_loss_list.append(running_loss/len(trainset))\n",
    "        train_acc_list.append(running_corrects.double().item()/len(trainset))\n",
    "        test_loss_list.append(test_loss/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(para, train_loss_list,label=\"train\")\n",
    "plt.scatter(para, test_loss_list,label=\"test\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('parameteres')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.53518,\n",
       " 0.57762,\n",
       " 0.60774,\n",
       " 0.6356,\n",
       " 0.68438,\n",
       " 0.69078,\n",
       " 0.70684,\n",
       " 0.73252,\n",
       " 0.73134,\n",
       " 0.73402,\n",
       " 0.74248]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5361, 0.5877, 0.5926, 0.6219, 0.6781, 0.6906]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_list.append(0.6811)\n",
    "test_acc_list.append(0.7019)\n",
    "test_acc_list.append(0.7217)\n",
    "test_acc_list.append(0.7149)\n",
    "test_acc_list.append(0.7443)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(para, train_acc_list,label=\"train\")\n",
    "plt.scatter(para, test_acc_list,label=\"test\")\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('parameteres')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
